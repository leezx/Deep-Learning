
Attention Is All You Need
https://arxiv.org/abs/1706.03762

# Tutorial

https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py

【Transformer代码(源码Pytorch版本)从零解读(Pytorch版本）】 https://www.bilibili.com/video/BV1dR4y1E7aL/?share_source=copy_web&vd_source=9d308488e379796a9ef7068283f7ceca

# QA                             
1. what is word embedding?
2. scratch-trained models and pre-trained models, the difference between these two? [pre-trained models are trained from large amounts of unlabeled data and then fine-tuned for specific tasks]
3. 通俗解释LLM里的layer normalization
4. residual connection是什么意思？如何通俗理解residual？
5. softmax函数在深度学习里的作用？为什么计算attention时用这个函数？
6. attention我知道，但什么是self-attention？
7. 为什么要MultiHead？
8. LLM模型的layer并不多，为什么会有几百亿个参数？
9. 以GPT为实例，给我分析模型的每个部分有多少参数？
10. 这么多参数，如何确保训练时会收敛，更新这些参数代价会不会太大？
11. 模型的训练是动态的，那么训练到什么程度才停止呢？
12. GPT是如何确定何时停止训练的？
