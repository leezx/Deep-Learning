
# Tutorial

https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py

【Transformer代码(源码Pytorch版本)从零解读(Pytorch版本）】 https://www.bilibili.com/video/BV1dR4y1E7aL/?share_source=copy_web&vd_source=9d308488e379796a9ef7068283f7ceca

# QA                             
1. what is word embedding?
2. scratch-trained models and pre-trained models, the difference between these two? [pre-trained models are trained from large amounts of unlabeled data and then fine-tuned for specific tasks]
3. 通俗解释LLM里的layer normalization
4. residual connection是什么意思？如何通俗理解residual？
5. softmax函数在深度学习里的作用？为什么计算attention时用这个函数？
6. attention我知道，但什么是self-attention？
7. 为什么要MultiHead？

